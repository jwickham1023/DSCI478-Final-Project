\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{setspace}
	\setstretch{1.6}
	
	\usepackage{tabularx}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Keep aspect ratio if custom image width or height is specified
    \setkeys{Gin}{keepaspectratio}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{soul}      % strikethrough (\st) support for pandoc >= 3.0.0
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{paper}
    
    
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@ges}{\let\PY@bf=\textbf\let\PY@it=\textit}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}

\title{Legacy NLP vs Modern Transformer Architecture Using DistilBERT}
\author{Chandler Grote, Nick Brady, Jakob Wickham}
\maketitle

    \subsubsection{Abstract}\label{abstract}

    Revisiting and improving upon prior research is how we advance
scientific understanding. We aimed to reproduce the Natural Language
Processing (NLP) methods described in Chen et al.(2016) with an updated
architecture. This paper explores practical advantages and benchmark
accuracy between current transformer NLP methods and legacy neural
network-based approaches. To create a controlled comparison, we
recreated the original data set using the methods described in the Git
repository (Chen, n.d.). We compared those results to modern transformer
architecture in NLP with Distilled Bidirectional Encoder Representations
from Transformers (DistilBERT), which would improve the accuracy of
previous NLP methods. The main challenge is that modern
transformer-based NLP has high hardware demands which require
workarounds, including dataset limits and infrastructure tradeoffs.
However, we achieved a peak accuracy score of 89.44\% using DistilBERT
while the legacy framework only achieved 23.09\% at its peak.

    \subsubsection{Key Terms}\label{key-terms}

\begin{samepage}
    \begin{itemize}
\tightlist
\item
  \textbf{Gated Recurrent Units (GRU)}: Lightweight neural networks that
  process sequences of words while retaining memory of previous inputs.
  Efficient and commonly used in earlier NLP systems. (Anishama, 2023)
\item
  \textbf{Transformer}: Modern architecture that simultaneously
  processes the entire input using self-attention, but has higher
  computational demands. (Aguilera, 2025)
\item
  \textbf{Attention}: A mechanism that allows the model to focus on the
  most essential parts of the context, improving decision-making by
  weighing its importance. (Winland, 2025)
\item
  \textbf{Bidirectional Encoder Representations from Transformers
  (BERT)}: A deep learning model using transformer-based architecture to
  understand sentence context by parsing left-to-right and
  right-to-left.
\item
  \textbf{Tokenization}: The process of breaking text into smaller units
  called tokens. Some models use a fixed token limit, which can force
  the shortening of long inputs, a process called truncation.
\end{itemize}
\end{samepage}

    \section{Introduction}\label{introduction}

    Natural Language Processing is a branch of machine learning focused on
enabling computers to understand and interpret human language. Both
methods used in this paper rely on a cloze-style question preprocessor,
shown in Figure 1. A cloze-style question is a fill-in-the-blank
question with one masked word, typically a named entity such as a
person, place, or thing. The model must predict the correct answer based
on the surrounding context. The two models we use in this paper differ
in executing this process. The legacy GRU framework masks the word,
constructs the cloze-style question, and ranks possible entity
candidates (Anishnama, 2023). The entity marker with the highest score
is selected as the predicted answer. The modern transformer framework
relies on token-level prediction based on a fixed vocabulary (Aguilera,
2025). It turns the task into a multiple-choice problem based on token
likelihoods. The versatility achieved by doing this comes at the cost of
significantly higher computational resources.

\begin{figure}
\begin{samepage}
	\begin{center}
		\textbf{Figure 1: An example of a cloze-style format}
	\end{center}
    \begin{Verbatim}[commandchars=\\\{\}]
Question: celebrity politician and former wrestler @placeholder had posed a
question concerning extraterrestrials to a government committee
Context: ( @entity2 ) the classic video game " @entity1 " was developed in
@entity3 back in the late 1970 's -- and now their real - life counterparts are
the topic of an earnest political discussion in @entity3 's corridors of power .
Label: @entity18
    \end{Verbatim}
    \vspace{1em}
\noindent\begin{minipage}{0.9\linewidth}
\begin{scriptsize}
\textbf{\textit{Figure description:}} \textit{This figure illustrates the cloze-style question format used in both the GRU and DistilBERT models. A named entity is masked within the question, and the model must infer the correct answer based on the surrounding context. This format is foundational to the CNN/Daily Mail datasets.}
\end{scriptsize}
\end{minipage}
\end{samepage}
\end{figure}

    To reduce computational demands, we used a distilled model. Distilled
models are smaller models, known as students, that are trained to mimic
behavior from a larger teacher model. BERT is trained by masking some
words in the text and then using the context in the text to the left and
right of the masked word to predict it. DistilBERT is a distillation of
BERT, which utilizes the same underlying process but runs on a smaller
and more efficient basis. By reducing the number of parameters,
DistilBERT can achieve similar goals on less intensive hardware while
maintaining comparable levels of accuracy.

Chen et al.(2016) introduced large-scale cloze-style question answering
tasks using new \emph{CNN} and \emph{Daily Mail} articles. Each article
included bullet point summaries with named entities. The model aims to
predict the masked entity by leveraging the article's context. They
implemented a recurrent neural network with GRU and an attention
mechanism to help align relevant parts of the article with the
questions, which was optimized with computational demand and
interoperability. Modern models like BERT and its distillations use
transformer architecture, which applies parallel self-attention rather
than sequential (Aguilera, 2025). These models are structurally
different and have different resource demands. Both models aim to map a
cloze-style question using the article context to a predicted entity.

    \section{Methodology}\label{methodology}

    Theano and Lasagne were implemented in the legacy model to reproduce the
original architecture by Chen et al.(2016). It could process full-length
articles without truncation, requiring minimal processing. This allowed
the model to operate on variable-length inputs without the need to
filter or constrain articles. Training was conducted using a batch size
of 96 on consumer-grade hardware, a Quadro RTX 3000 GPU. Due to modern
compatibility issues with Therano and Lasagne, reproducing the model
required a virtual Ubuntu machine with a legacy CUDA driver and
libraries to ensure compatibility and GPU acceleration. This GRU model
was trained from scratch without pretrained embedding or external
knowledge, making it a benchmark for evaluating models with minimal
dependencies.

Hugging Face Transformers and Pytorch were our baseline for the modern
model architecture. We selected distilbert-base-unccased as the
pretrained model architecture, and set floating-point-16 precision to
reduce memory usage. The preprocessing pipeline truncated each article
around the answer and filtered out samples with fewer than four entities
present. To prevent token overflow, we used an approximation of final
token counts. These adjustments excluded over 70\% of the data due to
token length constraints or a lack of entities. Even with this
significant data loss, the retained samples still reflected messy data
of real-world environments, as seen in Figure 2. DistilBert was trained
on an A100 GPU through Google Colab Pro using a batch size of 1 and no
gradient accumulation. The model was evaluated on a separate development
subset after each epoch.

Both models were trained using hyperparameters over three epochs to
maintain fairness. Evaluation relied on accuracy to focus on answer
correctness and aligned with the outputs of the legacy implementation.
Both models' data sets were constructed using Google DeepMind's question
story files from the NYU Archive (Cho, n.d.). The original
train/dev/test splits were preserved. By stratified random sampling,
80,000 training samples were selected from \emph{CNN} and \emph{Daily
Mail} articles to maintain article diversity and reduce computational
constraints on the transformer architecture.

\begin{samepage}
	\begin{center}
		\textbf{Figure 2: Entity count distribution}
	\end{center}
    \begin{Verbatim}[commandchars=\\\{\}]
     Dataset       Split  Filtered Count  Raw Max  Raw Mean  Raw Median
0        CNN       Train           25256      527     26.39        23.0
1        CNN  Validation            1376      187     26.19        22.0
2        CNN        Test            1007      394     24.22        21.0
3  DailyMail       Train           16019      329     26.25        22.0
4  DailyMail  Validation           16424      230     25.02        21.0
5  DailyMail        Test           12731      245     25.48        21.0
    \end{Verbatim}
    \vspace{1em}
\noindent\begin{minipage}{0.9\linewidth}
\begin{scriptsize}
\textbf{\textit{Figure description:}} \textit{Even with filtering out over 70\% of the original dataset to meet the tokenizer length requirements, the splits show decent variability. This shows that even after aggressive preprocessing, the data simulates how noisy real-world data is, and even with the smaller split sizes, the models will still be challenged.}
\end{scriptsize}
\end{minipage}
\end{samepage}

    \section{Results}\label{results}

\begin{samepage}
    During the training, there was a clear divide in each model's
convergence characteristics. DistilBERT showed a rapid improvement in
accuracy within the first thousand steps and increased over the
subsequent epochs. This is expected, as the model had an advantage from
extensive pretraining on the general domain. The GRU reached an early
plateau, showing limited gains after epoch one, even with stable
behavior during training. Training loss mirrored this trend, as
DistilBERT consistently reduced training loss over time while the GRU
model stabilized at a higher loss. DistilBERT's pretrained
initialization provided a significant head start, where the GRU
architecture training from scratch was limited by the limited
cloze-style data it was provided, see figures 3 and 4 for final
accuracy, training time, and hardware differences.
\end{samepage}
\newline{}

    \begin{samepage}
    \begin{center}
		\textbf{Figure 3: Accuracy comparisons between GRU and DistilBERT}
    \adjustimage{max size={0.6\linewidth}{0.6\paperheight}}{paper_files/paper_17_0.png}
    \end{center}

	\begin{center}
		\textbf{Figure 4: Table of performance and accuracies of GRU and DistilBERT}
	\end{center}
	\begin{table}[h]
\begin{scriptsize}
\begin{tabularx}{\textwidth}{l l l l l}
\textbf{Model} & \textbf{Dataset} & \textbf{Best Accuracy} & \textbf{Training Time} & \textbf{Notes} \\
\hline
GRU & CNN & 23.09\% & 5h 21m & Full context, consumer GPU \\
GRU & Daily Mail & 19.91\% & 10h 24m & Full context, consumer GPU \\
DistilBERT & CNN & 89.44\% & 4h 11m & Truncated input, A100 GPU \\
DistilBERT & Daily Mail & 86.79\% & 1h 51m & Truncated input, A100 GPU \\
\end{tabularx}
\end{scriptsize}
\caption{Comparison of GRU and DistilBERT performance on CNN and Daily Mail datasets.}
\end{table}
\vspace{1em}
\noindent\begin{minipage}{0.9\linewidth}
\begin{scriptsize}
\textbf{\textit{Figure descriptions:}} \textit{DistilBERT, as shown in both the table and the graph, achieves a much higher accuracy due to pretraining and having a smaller dataset to work with as well as dealing with smaller batch sizing, but at the cost of requiring more complicated preprocessing and intensive computational power, requiring an A100 GPU to be able to train the models.}
\end{scriptsize}
\end{minipage}
\end{samepage}
{ \hspace*{\fill} \\}

\begin{samepage}
    Superior performance was achieved with DistilBERT in both datasets, but
it came at significant tradeoffs. This model required high-end hardware,
aggressive preprocessing, and exclusion of most of the dataset to meet
memory constraints. While the GRU model ran entirely on consumer-grade
hardware, it had minimal processing and could use full-length article
context. The GRU model was less accurate, but its ability to be used and
its interpretability make it a consideration with limited resources or
technical requirements
\end{samepage}

    \section{Conclusion}\label{conclusion}

    This exploration shows one of the central debates in NLP: the newest
model isn't always the right tool. DistilBERT's performance is four
times greater than the GRU model's, but this comes at the cost of
extensive infrastructure, massive data pruning, and reliance on
pre-trained language, showing the power and limitations of modern
transformer-based architecture. Still, it shows the growing barrier for
anyone without access to high-end computing. Despite being made nine
years ago, the GRU model has proven capable of handling long-form input
and offering relative ease of deployment. It is compatible with unfitted
full article text while operating in resource-constrained environments.
These traits, simplicity, transparency, and compatibility, offer
meaningful advantages for resource-limited settings. These often get
lost due to the ever-increasing need for benchmark improvements.
Ultimately, these tools are only as good as the context in which they
are used. Data scientists must understand when and why to use them,
choosing tools that align with the problem, its constraints, and
intended impact. Revisiting older models isn't for nostalgia. It's about
expanding our perspective and re-evaluating which solution meets our
goals. As NLP continues to evolve, looking back can still show us paths
forward.

\section{References}\label{references}

Chen, D., Bolton, J., \& Manning, C. D. (2016). A thorough examination
of the CNN/Daily Mail Reading Comprehension Task. \emph{Proceedings of
the 54th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers)}. https://doi.org/10.18653/v1/p16-1223

Chen, D. (n.d.). \emph{Danqi/RC-CNN-dailymail: CNN/Daily Mail Reading
Comprehension task}. GitHub. https://github.com/danqi/rc-cnn-dailymail

Aguilera, F. M. (2025, April 18). \emph{Transformer-based multiple
choice question answering: Implementation, output analysis, and
bias\ldots{}} Medium.
https://medium.com/ai-simplified-in-plain-english/transformer-based-multiple-choice-question-answering-implementation-output-analysis-and-bias-e04d3d6d9c03

Winland, V. (2025, March 11). \emph{What is self-attention?}. IBM.
https://www.ibm.com/think/topics/self-attention

Anishnama. (2023, May 4). \emph{Understanding gated recurrent unit (GRU)
in deep learning}. Medium.
https://medium.com/@anishnama20/understanding-gated-recurrent-unit-gru-in-deep-learning-2e54923f3e2

Cho, K. (n.d.). \emph{DeepMind Q\&A Dataset}. DMQA.
https://cs.nyu.edu/\textasciitilde kcho/DMQA/


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
